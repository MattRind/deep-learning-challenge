## **Summary Report**
# deep-learning-challenge 
1. **Overview** of the analysis: To create a tool that will help the nonprofit foundation Alphabet Soup select applicants for funding with the best chance of success in their ventures. In this case the tool uses a binary classifier as part of a machine learning and neural network model, and the source of data is from more than 34,000 organizations that have received funding from Alphabet Soup.

2. **Results:** 

* Data Preprocessing

  - The variable that was assigned as the target in the neural network model was 'IS_SUCCESSFUL'. 
  - There are 9 features for my model: 'APPLICATION_TYPE', 'AFFILIATION',	'CLASSIFICATION',	'USE_CASE',	'ORGANIZATION',	'STATUS',	'INCOME_AMT',	'SPECIAL_CONSIDERATIONS' and	'ASK_AMT'. Two features are numeric and 7 are categorical. Two of the categorical features were bins to reduce total features after one hot encoding.
  - The variables that were removed from the input data because they are neither targets nor features were two identification fields of data 'EIN' and 'NAME'. As explained in the summary, 'STATUS' and 'SPECIAL_CONSIDERATIONS' could be removed in further neural network modeling.

* Compiling, Training, and Evaluating the Model

- Initial model `AlphabetSoupCharity.ipynb`. There were a grand total of 4 models complied, trained and evaluated. In the original model, there were two hidden layers; the first layer had 80 neurons and the 2nd layer 30. The activation function for the hidden layers was 'relu', and the activation function for the output layer was 'sigmoid'. The number of input features was 43, and total parameters for the modelling was 5981. Even though the number of neurons was based on the given template, the total matches the rule of thumb saying that there should be 2–3 times as many neurons as there are input features. 'Relu' was selected for the activation function for the hidden layers because this type is very effective in converting linear combinations and relatively simple to implement for a model like this one. Also the number of hidden layers was based on simply a good place to start. For training the model, the batch size was 32 and number of epochs was 100. The model accuracy was 72.62%, falling short of the 75% goal. The loss of the model was 0.5684.

- Optimization models `AlphabetSoupCharity_Optimization.ipynb`. A total of 3 optimization models were run since each successive model did not reach the accuracy objective. Additional data preprocessing was performed: Two categorical features were converted to numeric. The data in the original 'INCOME_AMT' column showed a range of incomes; it was converted to a minimum income based on the lowest value of each range. 'SPECIAL_CONSIDERATIONS' was converted to binary data. This meant that the number of input features was reduced to 34 from 43. 
 - The first optimization did not change any other hyperparameter. After the first optimization trial was performed, the model accuracy increased slightly to 73.44%. Similarly to the original model, the loss based on the training set converges to 0.54-0.55, whereas the loss based on the test set is higher and does not reduce from a 0.565 - 0.575 range.
 - The second optimization model added a third hidden layer and increased the number of neurons from 80 to 90, based on the fact that the loss for both the training and test set were not reducing significantly. The other change was the number of epochs was increased to 200 from 100. The model accuracy slightly decreased to 73.0% after the increase of neurons and parameters in the model. Loss for both the training and test set get worse with additional epoch, and the trend shows that the model was overfitted.
 - The third optimization model increased the number of neurons for the second hidden layer to 90 from 30, but kept the number of neurons for the added third layer at 30. The number of epochs was set back to 100 to avoid an overfitting issue. The model accuracy of the third optimization trial was 73.25%. Similarly to the second trial the loss trended worse as epochs increased and overfitting was apparent.

3. **Summary:** Overall, the optimization models only slightly improved the accuracy from the initial model, however none of them reached the target 75%. Overfitting was evident when more neurons were used in the model. Recommend reducing the number of input features even further, such as eliminating data that perhaps is not useful to the purpose of the study in the first place; this includes dropping 'STATUS' and 'SPECIAL_CONSIDERATIONS' from the analysis. The next thing to fix is the overfitting issue by going back to 2 hidden layers and but keeping the number of neurons 2–3 times as there are input features. The final hyperparameter to modify is the activation function. Recommend to investigate using either 'leaky relu' or 'relu6' as an activation function for one of the hidden layers. Despite the large number of parameters, the activation functions may not be that effective. For example 'relu6' encourages the model to learn sparse input earlier. 